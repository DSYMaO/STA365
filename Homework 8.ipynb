{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bb9107",
   "metadata": {},
   "source": [
    "# Homework 8: (a) Posterior Predictive Distributions<br> and (b) Missing Data Imputation\n",
    "\n",
    "### 1. Describe how the posterior predictive distribution is created for mixture models \n",
    "\n",
    "### 2. Describe how the posterior predictive distribution is created in general\n",
    "\n",
    "### 3. Have glance through [this](https://www.pymc.io/projects/examples/en/latest/case_studies/Missing_Data_Imputation.html) and then describe how, if you were doing a regression of $y$ on $X$ but $X$ had some missing values, you could perform a Bayesian analysis without throwing away the rows with missing values in $X$\n",
    "\n",
    "- **Hint: latent variables $v$ indicating the subpopulation are competely missing values that we simply treat as paramters to be inferred though posterior analysis... the same sort of thing can be done with missing values in data that need to be imputed... we should just be careful about the MCAR assumption...**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0f3ba",
   "metadata": {},
   "source": [
    "# 1) The posterior predictive distribution represents a combination of parameters which is drawn from the posterior distribution repeatedly to generate the mixture model which is like Gibbs sampling where we approximate the model from drawing samples. This is useful when the data is multi-modal.\n",
    "\n",
    "2) The posterior predictive distribution can generally be created through drawing from a random sample to attain a portion of the distribution for the mixture model and having these steps being repeated over and over to \"map out\" the posterior predictive distribution.\n",
    "\n",
    "3) I believe the idea is that with a scalar mean, missing values would have the same mean and standard devation in the sampling distribution to give its implied distribution. It is fine when the type of missingness is when it is Missing Completely at Random or at random. But you could also partition the data into patterns of “missing-ness” and treating each partition as contributing to the ultimate log-likelihood to maximize and then use that implied distribution to sample from and perrm Bayesian analysis with the regresion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa43c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
